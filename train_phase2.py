import os
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from tqdm import tqdm

# ç¡®ä¿å¼•ç”¨æ­£ç¡®çš„æ¨¡å—
from data.dataset import BEVMultiTaskDataset
from models.detector import WBEVFusionNet  
from utils.losses import WBEVLoss           
from configs.config import BEVConfig as cfg

def train_phase2():
    # 1. è®¾ç½®è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸš€ Starting Phase 2 Resume Training on {device}...")

    # 2. åˆå§‹åŒ–æ¨¡å‹
    model = WBEVFusionNet(cfg).to(device)

    # 3. åŠ è½½é»„é‡‘å­˜æ¡£ e60 (MAE 2.3m)ï¼Œä¸è¦ç”¨ e70 ç»§ç»­ç»ƒ
    checkpoint_path = './checkpoints/phase2_resumed_e60.pth'
    start_epoch = 60

    if os.path.exists(checkpoint_path):
        print(f"ğŸ“¦ Resuming from golden checkpoint: {checkpoint_path}")
        checkpoint = torch.load(checkpoint_path, map_location=device)
        
        # æå– state_dict
        if 'model_state_dict' in checkpoint:
            sd = checkpoint['model_state_dict']
        else:
            sd = checkpoint
            
        # ç§»é™¤å¯èƒ½å­˜åœ¨çš„ module. å‰ç¼€
        sd = {k.replace('module.', ''): v for k, v in sd.items()}
        
        # ä¸¥æ ¼åŠ è½½ (strict=True)ï¼Œå› ä¸ºæˆ‘ä»¬è¦æ¥ç»­è®­ç»ƒï¼Œç»“æ„å¿…é¡»å®Œå…¨ä¸€è‡´
        model.load_state_dict(sd, strict=True)
        print("âœ… Healthy weights loaded successfully.")
    else:
        print(f"âŒ Error: {checkpoint_path} not found! Cannot resume.")
        return

    # 4. æ•°æ®å‡†å¤‡
    train_dataset = BEVMultiTaskDataset(cfg.DATA_ROOT, split='train')
    train_loader = DataLoader(
        train_dataset, 
        batch_size=cfg.BATCH_SIZE, 
        shuffle=True, 
        num_workers=4,  # å·²ç»ç¨³å®šï¼Œå¯ä»¥å¼€å¯å¤šè¿›ç¨‹
        collate_fn=BEVMultiTaskDataset.collate_fn
    )

    # 5. ä¼˜åŒ–å™¨ã€è°ƒåº¦å™¨ä¸æŸå¤±
    # æåº¦ä¿å®ˆï¼š1e-6 åƒç»£èŠ±ä¸€æ ·å¾®è°ƒï¼ˆå¤šé¡¹å¼ç³»æ•°ææ•æ„Ÿï¼‰
    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-6, weight_decay=1e-2)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', patience=3, factor=0.5
    )

    criterion = WBEVLoss()

    # 6. è®­ç»ƒå¾ªç¯
    total_epochs = 100 # åœ¨ e50 åŸºç¡€ä¸Šå†è·‘ 50 è½®
    model.train()
    
    print(f"ğŸ“ˆ Resuming from Golden Checkpoint e60. Target: Stabilize MAE < 2m.")
    
    for epoch in range(start_epoch, total_epochs):
        epoch_loss = 0
        pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{total_epochs}")
        
        for imgs, points, targets in pbar:
            # æ•°æ®ç§»è‡³ GPU
            imgs = imgs.to(device)
            points = [p.to(device) for p in points]

            # å¤„ç† targetsï¼šcollate_fn è¿”å› list of dicts
            if isinstance(targets, list):
                new_targets = []
                for item in targets:
                    if isinstance(item, dict):
                        new_targets.append({k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in item.items()})
                    else:
                        new_targets.append(item.to(device) if isinstance(item, torch.Tensor) else item)
                targets = new_targets
            elif isinstance(targets, dict):
                for k, v in targets.items():
                    if isinstance(v, torch.Tensor):
                        targets[k] = v.to(device)

            optimizer.zero_grad()

            # å‰å‘ä¼ æ’­
            preds = model(imgs, points)

            # è®¡ç®—æŸå¤±
            loss_dict = criterion(preds, targets)

            # æ ¹æ® WBEVLoss çš„å®é™…è¿”å›é”®åå–å€¼ï¼Œå…¼å®¹ total_loss / loss
            loss = loss_dict.get('total_loss', loss_dict.get('loss'))
            if loss is None:
                raise KeyError(f"æ— æ³•åœ¨ loss_dict ä¸­æ‰¾åˆ°æŸå¤±é”®ã€‚å½“å‰é”®åä¸º: {list(loss_dict.keys())}")

            if torch.isnan(loss):
                print("âŒ NaN Loss detected! Skipping batch.")
                continue

            loss.backward()

            # æ¢¯åº¦è£å‰ªæ›´ä¸¥æ ¼ (0.5)ï¼Œé˜²æ­¢åæ•°æ®æ—¶ç³»æ•°è¢«çªç„¶è¸¢é£
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)

            optimizer.step()

            epoch_loss += loss.item()

            # åŠ¨æ€ç›‘æ§ï¼šé”®åä¸ loss_dict å¯¹åº”
            pbar.set_postfix({
                "Total": f"{loss.item():.3f}",
                "Poly": f"{loss_dict.get('poly_loss', loss_dict.get('l_poly', 0)):.4f}",
                "Seg": f"{loss_dict.get('l_seg', loss_dict.get('seg_loss', 0)):.3f}"
            })

        avg_loss = epoch_loss / len(train_loader)
        scheduler.step(avg_loss)

        # æ¯ 5 è½®ä¿å­˜ä¸€æ¬¡ï¼Œæ›´å¤šæœºä¼šæ•æ‰å¥½æ¨¡å‹
        if (epoch + 1) % 5 == 0:
            os.makedirs('checkpoints', exist_ok=True)
            save_path = f'checkpoints/phase2_resumed_e{epoch+1}.pth'
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict(),
                'loss': avg_loss
            }, save_path)
            print(f"ğŸ’¾ Saved checkpoint: {save_path} (avg_loss={avg_loss:.4f})")

if __name__ == "__main__":
    train_phase2()