"""
fix_checkpoint_keys.py - ä¿®å¤å±‚å¯¼å…¥é—®é¢˜ï¼Œç”Ÿæˆå¯æ­£ç¡®åŠ è½½çš„ checkpoint

ä½¿ç”¨åŸºäºŽå±‚çº§åç§»çš„ç¡¬æ˜ å°„ï¼Œé¿å…å½¢çŠ¶åŒ¹é…å¯¼è‡´ cls/reg/mask å¼ å† æŽæˆ´ã€‚
Phase 1 checkpoint (ç›´æŽ¥ Conv2d) -> Phase 2 æ¨¡åž‹ (Sequential å†… .2 ä¸ºè¾“å‡ºå±‚)
"""
import os
import sys
import torch

project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from configs.config import BEVConfig as cfg
from models.detector import WBEVFusionNet

# ç¡¬ç¼–ç æ˜ å°„ï¼šckpt_key -> model_keyï¼Œåªåšåç§°/å±‚çº§è½¬æ¢ï¼Œä¸ä¾èµ–å½¢çŠ¶åŒ¹é…
# Phase 1: ç›´æŽ¥ cls_head/reg_head, seg_head
# Phase 2 æ¨¡åž‹: cls_head.2/reg_head.2 ä¸ºè¾“å‡ºå±‚, mask_head ä¸Ž seg_head ç­‰ä»·
FIX_MAP = {
    # åˆ†å‰²å¤´ï¼šckpt ç”¨ seg_headï¼Œéƒ¨åˆ† Phase 2 æ¨¡åž‹ç”¨ mask_head
    "head.seg_head.0.weight": "head.seg_head.0.weight",
    "head.seg_head.0.bias": "head.seg_head.0.bias",
    "head.seg_head.1.weight": "head.seg_head.1.weight",
    "head.seg_head.1.bias": "head.seg_head.1.bias",
    "head.seg_head.1.running_mean": "head.seg_head.1.running_mean",
    "head.seg_head.1.running_var": "head.seg_head.1.running_var",
    "head.seg_head.1.num_batches_tracked": "head.seg_head.1.num_batches_tracked",
    "head.seg_head.3.weight": "head.seg_head.3.weight",
    "head.seg_head.3.bias": "head.seg_head.3.bias",
    # æ£€æµ‹å¤´ï¼šPhase 1 ç›´æŽ¥è¾“å‡º -> Phase 2 Sequential çš„ .2 è¾“å‡ºå±‚
    # è‹¥æ¨¡åž‹ä¸ºç›´æŽ¥ Conv2dï¼Œç”¨ä¸‹æ–¹ USE_DIRECT_HEAD åˆ‡æ¢
    "head.cls_head.weight": "head.cls_head.weight",
    "head.cls_head.bias": "head.cls_head.bias",
    "head.reg_head.weight": "head.reg_head.weight",
    "head.reg_head.bias": "head.reg_head.bias",
}

# è‹¥ç›®æ ‡æ¨¡åž‹ä½¿ç”¨ Sequential cls_head/reg_headï¼ˆ.0, .2 ç»“æž„ï¼‰ï¼Œå¯ç”¨æ­¤æ˜ å°„
FIX_MAP_SEQUENTIAL_HEAD = {
    "head.cls_head.weight": "head.cls_head.2.weight",
    "head.cls_head.bias": "head.cls_head.2.bias",
    "head.reg_head.weight": "head.reg_head.2.weight",
    "head.reg_head.bias": "head.reg_head.2.bias",
}

# è‹¥ç›®æ ‡æ¨¡åž‹ç”¨ mask_head è€Œéž seg_head
FIX_MAP_MASK_HEAD = {
    "head.seg_head.0.weight": "head.mask_head.0.weight",
    "head.seg_head.0.bias": "head.mask_head.0.bias",
    "head.seg_head.1.weight": "head.mask_head.1.weight",
    "head.seg_head.1.bias": "head.mask_head.1.bias",
    "head.seg_head.1.running_mean": "head.mask_head.1.running_mean",
    "head.seg_head.1.running_var": "head.mask_head.1.running_var",
    "head.seg_head.1.num_batches_tracked": "head.mask_head.1.num_batches_tracked",
    "head.seg_head.3.weight": "head.mask_head.3.weight",
    "head.seg_head.3.bias": "head.mask_head.3.bias",
}


def get_fix_map(use_sequential_head=True, use_mask_head=True):
    """èŽ·å–åˆå¹¶åŽçš„æ˜ å°„è¡¨ï¼Œä¾› debug ç­‰è„šæœ¬ä½¿ç”¨"""
    m = dict(FIX_MAP)
    if use_sequential_head:
        m.update(FIX_MAP_SEQUENTIAL_HEAD)
    if use_mask_head:
        m.update(FIX_MAP_MASK_HEAD)
    return m


def fix_checkpoint(ckpt_path: str, output_path: str = None, use_sequential_head: bool = True, use_mask_head: bool = True):
    """
    use_sequential_head: Phase 2 æ¨¡åž‹ cls/reg_head ä¸º Sequentialï¼Œè¾“å‡ºåœ¨ .2 å±‚
    use_mask_head: Phase 2 æ¨¡åž‹ç”¨ mask_head è€Œéž seg_head
    """
    print(f"ðŸ› ï¸ æ­£åœ¨ä¿®å¤æƒé‡æ–‡ä»¶: {ckpt_path}")
    checkpoint = torch.load(ckpt_path, map_location='cpu')
    state_dict = checkpoint.get('model_state_dict', checkpoint.get('model', checkpoint.get('state_dict', checkpoint)))
    if not isinstance(state_dict, dict):
        print("âŒ checkpoint ä¸­æœªæ‰¾åˆ°æœ‰æ•ˆçš„ state_dict")
        return False

    state_dict = {k.replace('module.', ''): v for k, v in state_dict.items()}

    # åˆå¹¶æ˜ å°„è¡¨
    fix_map = dict(FIX_MAP)
    if use_sequential_head:
        fix_map.update(FIX_MAP_SEQUENTIAL_HEAD)
    if use_mask_head:
        fix_map.update(FIX_MAP_MASK_HEAD)

    new_state_dict = {}
    mapping_log = []

    for ckpt_k, v in state_dict.items():
        model_k = fix_map.get(ckpt_k, ckpt_k)

        # åªåš key å˜æ¢ï¼Œä¸åšå½¢çŠ¶æ ¡éªŒï¼ˆé¿å… cls/reg äº’æ¢ï¼‰
        new_state_dict[model_k] = v
        if model_k != ckpt_k:
            mapping_log.append((ckpt_k, model_k))

    # æ ¡éªŒï¼šå½“è¾“å‡ºå…¼å®¹å½“å‰æ¨¡åž‹æ—¶å°è¯•åŠ è½½ï¼ˆuse_sequential_head/use_mask_head ä¼šæ”¹å˜ keyï¼Œå¯èƒ½ä¸å…¼å®¹ï¼‰
    model = WBEVFusionNet(cfg)
    try:
        msg = model.load_state_dict(new_state_dict, strict=False)
        if msg.missing_keys:
            print(f"\nâš ï¸ å½“å‰æ¨¡åž‹ç¼ºå¤± {len(msg.missing_keys)} å±‚ï¼ˆè¾“å‡ºå¯èƒ½é¢å‘ use_sequential_head/use_mask_head æ¨¡åž‹ï¼‰")
    except Exception:
        pass

    fix_count = len(mapping_log)
    print(f"âœ… ä¿®å¤å®Œæˆï¼")
    print(f"ðŸ“Š æ€»è®¡é‡å‘½å: {fix_count} å±‚ | å…± {len(new_state_dict)} å±‚")
    if mapping_log:
        print("\nðŸ“‹ Key æ˜ å°„è®°å½•:")
        for a, b in mapping_log[:20]:
            print(f"   {a} -> {b}")
        if len(mapping_log) > 20:
            print(f"   ... åŠ {len(mapping_log) - 20} æ¡")

    out = output_path or ckpt_path.replace('.pth', '_fixed.pth')
    save_ckpt = {
        'model_state_dict': new_state_dict,
        **{k: v for k, v in checkpoint.items() if k not in ('model_state_dict', 'model', 'state_dict')},
    }
    torch.save(save_ckpt, out)
    print(f"\nðŸ’¾ å·²ä¿å­˜: {out}")
    return True


def _get_ckpt_dir():
    if os.path.exists(cfg.CKPT_DIR):
        return cfg.CKPT_DIR
    root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    return os.path.join(root, "checkpoints")


if __name__ == "__main__":
    import glob

    pos_args = [a for a in sys.argv[1:] if not a.startswith("--")]

    ckpt_dir = _get_ckpt_dir()
    ckpt_path = pos_args[0] if len(pos_args) >= 1 else None
    out_path = pos_args[1] if len(pos_args) >= 2 else None

    if not ckpt_path:
        for name in ["model_e80.pth", "model_e60.pth"]:
            cand = os.path.join(ckpt_dir, name)
            if os.path.exists(cand):
                ckpt_path = cand
                break
        if not ckpt_path:
            files = glob.glob(os.path.join(ckpt_dir, "*.pth"))
            if files:
                ckpt_path = max(files, key=os.path.getctime)

    if not ckpt_path or not os.path.exists(ckpt_path):
        print("ç”¨æ³•: python fix_checkpoint_keys.py <checkpoint_path> [output_path] [--sequential-head] [--mask-head]")
        print("  --no-sequential-head  ç¦ç”¨ cls/reg -> .2 æ˜ å°„ï¼ˆç›®æ ‡ä¸ºç›´æŽ¥ Conv2dï¼‰")
        print("  --no-mask-head        ç¦ç”¨ seg_head -> mask_head æ˜ å°„")
        sys.exit(1)

    use_seq = "--no-sequential-head" not in sys.argv  # é»˜è®¤ True
    use_mask = "--no-mask-head" not in sys.argv  # é»˜è®¤ Trueï¼ˆç›®æ ‡æ¨¡åž‹ç”¨ mask_headï¼‰
    fix_checkpoint(ckpt_path, out_path, use_sequential_head=use_seq, use_mask_head=use_mask)
